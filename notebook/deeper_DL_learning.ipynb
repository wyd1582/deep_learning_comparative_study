{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook\n",
    "- understand the boundary of DL and other concepts\n",
    "- able to build a DL NN\n",
    "- validate DL model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "from utils.model import *\n",
    "from utils.opt_func import *\n",
    "from utils.testCases import *\n",
    "from config import nn_config\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#import skimage\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is DL \n",
    "\n",
    "Deep learning = artificial neural network (ANN) with >1 hidden layer\n",
    "\n",
    "for example shallow neural network for a binary target:\n",
    "\n",
    "| Input Layer  | Hidden Layer | Output Layer |\n",
    "| ----------- | ----------- |----------- | \n",
    "| $x_1, x_2$  | unit1 = $\\beta_{01} + \\beta_{11}x_1+\\beta_{12}x_2$|$\\beta_0+\\beta_1\\tanh(unit_1)+\\beta_2\\tanh(unit_2)+ \\beta_3\\tanh(unit_3)$|\n",
    "|   |   unit2 =  $\\beta_{02} + \\beta_{12}x_1 + \\beta_{22}x_2$   |    |\n",
    "|   | unit3 =  $\\beta_{03} + \\beta_{13}x_1 +\\beta_{23}x_2$      |    |\n",
    "\n",
    "formula：\n",
    "\n",
    "$logit(E(y)) = \\beta_0+\\beta_1\\tanh(\\beta_{01} + \\beta_{11}x_1+\\beta_{12}x_2)+\\beta_2\\tanh(\\beta_{02} + \\beta_{12}x_1 + \\beta_{22}x_2)+ \\beta_3\\tanh(\\beta_{03} + \\beta_{13}x_1 +\\beta_{23}x_2)$\n",
    "\n",
    "loss function:\n",
    "\n",
    "$ \\hat{\\beta} = argmin_{\\beta}-\\sum^n_{i=1} y_i ln(\\hat{y_i}(\\beta)) +  (1-y_i)ln(1-\\hat{y_i}(\\beta)) $\n",
    "\n",
    "\n",
    "for a deep neural network for binary target:\n",
    "\n",
    "\n",
    "| Input Layer  | Hidden Layer 1 | Hidden Layer 2 | Output Layer |\n",
    "| ----------- | ----------- |----------- |----------- | \n",
    "| $x_1, x_2$  | unit_{11} = $\\beta_{011} + \\beta_{111}x_1+\\beta_{211}x_2$| unit_1 = $\\beta_{012} + \\beta_{112}\\tanh(unit_{11})+\\beta_{212}\\tanh(unit_{12})+ \\beta_{312}\\tanh(unit_{13})$ | $\\beta_0 +\\beta_1 \\tanh (unit_1) + \\beta_2 \\tanh(unit_2)$  |\n",
    "|   |   unit_{12} = $\\beta_{021} + \\beta_{121}x_1+\\beta_{221}x_2$   |  unit_2 = $\\beta_{022} + \\beta_{122}\\tanh(unit_{11})+\\beta_{222}\\tanh(unit_{12})+ \\beta_{322}\\tanh(unit_{13})$  ||\n",
    "|   | unit_{13} = $\\beta_{031} + \\beta_{131}x_1+\\beta_{231}x_2$      |    | |\n",
    "\n",
    "\n",
    "formula：\n",
    "\n",
    "$logit(E(y)) = \\beta_0+\\beta_1 \\big\\{ \\tanh(\\beta_{012} + \\beta_{112}\\tanh(unit_{11})+\\beta_{212}\\tanh(unit_{12})+ \\beta_{312}\\tanh(unit_{13}))+\\beta_2 \\big\\{ \\tanh(\\beta_{022} + \\beta_{122}\\tanh(unit_{11})+\\beta_{222}\\tanh(unit_{12})+ \\beta_{322}\\tanh(unit_{13}))$\n",
    "\n",
    "loss function:\n",
    "\n",
    "$ \\hat{\\beta} = argmin_{\\beta}-\\sum^n_{i=1} y_i ln(\\hat{y_i}(\\beta)) +  (1-y_i)ln(1-\\hat{y_i}(\\beta)) $\n",
    "\n",
    "\n",
    "- reflect to other mathemetical algorithms and models?\n",
    "  - sequencial model, feature learning, activation function, kernel regression\n",
    "  \n",
    "- what is the typical training process ?\n",
    "  - initialization : \n",
    "    get $\\beta^{0},\n",
    "  \n",
    "  - parameter setting:\n",
    "      \n",
    "      weghts = ${\\beta_1, \\beta_{11}, \\beta_{21}, \\beta_{11}, \\beta_{2}, \\beta_{12}, \\beta_{22}, \\beta_{3}, \\beta_{13}, \\beta_{23}}$\n",
    "      \n",
    "      biases = ${\\beta_0, \\beta_{01}, \\beta_{02}, \\beta_{03}}$\n",
    "  \n",
    "  - optimization:\n",
    "      \n",
    "    $\\hat{\\beta} = argmin_{\\beta} \\epsilon (\\beta)$\n",
    "        \n",
    "    $ \\beta^{(k+1)} = \\beta^{(k)} + \\epsilon^{(k)} $ (step size and direction) \n",
    "  \n",
    "  \n",
    "  \n",
    "- best use cases?\n",
    "  - NLP, imagine recognition etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# slide and plots for demo here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prepare data for DL\n",
    "- dimension\n",
    "- train, test, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read data\n",
    "DATA_PATH = '/Users/yudiwang/Desktop/playground/deep_learning_comparative_study/exercises/data/'\n",
    "\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data(DATA_PATH)\n",
    "\n",
    "# check data dimension\n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a DL Network?\n",
    "- NN\n",
    "- CNN\n",
    "- RNN\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function\n",
    "- Regression: MSE, RMSE\n",
    "- Binary: log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ML to Opt for DNN\n",
    "Goal is to minimize expected loss on training set\n",
    "- SGD \n",
    "- Local minima myth from spurious surface of DNN\n",
    "- AdaGrad, RMSProp, Adam : difference in adaptivity\n",
    "- Bring in Hession for global opt: LBFGS \n",
    "\n",
    "$\\hat{\\beta} = argmin_{\\beta} \\epsilon (\\beta)$\n",
    "        \n",
    "$ \\beta^{(k+1)} = \\beta^{(k)} + \\delta^{(k)} $ (step size and direction) \n",
    "\n",
    "- batch gradient descent, first order: \n",
    "$ \\delta^{(k)}  = -\\eta \\frac{\\nabla\\epsilon(\\beta^{(k)})}{\\nabla\\beta} $\n",
    "\n",
    "- backpropagation, first order:\n",
    "$ \\delta^{(k)} = -\\eta \\frac{\\nabla\\epsilon(\\beta^{(k)})}{\\nabla\\beta} + \\alpha \\delta^{(k-1)}$\n",
    "\n",
    "- modiofied Newton\n",
    "$ \\delta^{(k)} = -\\rho^{(k)} \\big\\{\\frac{\\delta^2\\epsilon(\\beta^{(k)})}{\\delta\\beta\\delta\\beta'} \\big\\}^{(-1)} \\frac{\\delta \\epsilon(\\beta^{(k)})}{\\delta\\beta} $\n",
    "\n",
    "- quasi-Newton\n",
    "$ \\delta^{(k)} = -\\rho^{(k)} B^{(k)} \\frac{\\delta \\epsilon(\\beta^{(k)})}{\\delta\\beta} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1: initial parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00865408 -0.02301539]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(2,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: forward computing for feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n"
     ]
    }
   ],
   "source": [
    "#forward computing\n",
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "#compute cost\n",
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(loss_function(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4: backward computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#backward computing\n",
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5: update parameter and convert to optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "#update parameters\n",
    "\n",
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 6: define model and train and get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6972515266984297\n",
      "Cost after iteration 100: 0.6258189987064959\n",
      "Cost after iteration 200: 0.5605784060355862\n",
      "Cost after iteration 300: 0.47831279192181125\n",
      "Cost after iteration 400: 0.4526603729238983\n",
      "Cost after iteration 500: 0.4117255706045356\n",
      "Cost after iteration 600: 0.3852504336082606\n",
      "Cost after iteration 700: 0.3231118569806582\n",
      "Cost after iteration 800: 0.27645830994028087\n",
      "Cost after iteration 900: 0.2298164725400055\n",
      "Cost after iteration 1000: 0.20493986146142718\n",
      "Cost after iteration 1100: 0.14833542043020975\n",
      "Cost after iteration 1200: 0.12379884037384455\n",
      "Cost after iteration 1300: 0.3627589556997483\n",
      "Cost after iteration 1400: 0.09626278773471086\n",
      "Cost after iteration 1500: 0.08190948311294277\n",
      "Cost after iteration 1600: 0.07132282993992237\n",
      "Cost after iteration 1700: 0.06314932895996458\n",
      "Cost after iteration 1800: 0.05612670144299609\n",
      "Cost after iteration 1900: 0.04983386622727822\n",
      "Cost after iteration 2000: 0.04518876152270858\n",
      "Cost after iteration 2100: 0.041244812943796526\n",
      "Cost after iteration 2200: 0.03783452695833238\n",
      "Cost after iteration 2300: 0.03483988113520803\n",
      "Cost after iteration 2400: 0.032137927449854276\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# could be furtherly improve for tuning\n",
    "\n",
    "#put things together\n",
    "\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation='sigmoid')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation='sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = loss_function(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation='sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 8: make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999999999999998\n",
      "Accuracy: 0.72\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (1,50) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7ade3d5da512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint_mislabeled_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/playground/deep_learning_comparative_study/notebook/utils/opt_func.py\u001b[0m in \u001b[0;36mprint_mislabeled_images\u001b[0;34m(classes, X, y, p)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \"\"\"\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0mmislabeled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m40.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set default size of plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (1,50) "
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_train = predict(train_x, train_y, parameters)\n",
    "\n",
    "\n",
    "pred_test = predict(test_x, test_y, parameters)\n",
    "\n",
    "\n",
    "print_mislabeled_images(classes, test_x, test_y, pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result analysis and potential problems\n",
    "- test error too large : due to feature transformation not viable enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform fine tuning\n",
    "- learning rate - loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE_LIST = [0.05, 0.075, 0.1]\n",
    "pred_test = []\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    acc = np.sum((p == y)/m)\n",
    "    print(\"Accuracy: \"  + str(acc))\n",
    "        \n",
    "    return p, acc\n",
    "                 \n",
    "for lr in LEARNING_RATE_LIST:\n",
    "    parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), learning_rate = lr, num_iterations = 2500, print_cost=False)\n",
    "    pred_test.append(predict(test_x, test_y, parameters)[1])\n",
    "    \n",
    "print(pred_test)\n",
    "# plot the cost\n",
    "plt.plot(LEARNING_RATE_LIST, pred_test)\n",
    "plt.ylabel('prediction accuracy')\n",
    "plt.xlabel('learning rate)')\n",
    "plt.title(\"Learning rate =\" + str(LEARNING_RATE_LIST))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a CNN model to improve model performance !\n",
    "  \n",
    " - best for grid like topology 2D image\n",
    " - convolution and pooling\n",
    " - slide filter and feature map, sparse interaction\n",
    " - design structure\n",
    " - image recognition and pattern recognition etc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Convolution functions, including:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward\n",
    "    - Convolution backward (optional)\n",
    "- Pooling functions, including:\n",
    "    - Pooling forward\n",
    "    - Create mask \n",
    "    - Distribute value\n",
    "    - Pooling backward (optional)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_pad(X, pad):\n",
    "    '''\n",
    "    Padding for height and width for convolutional layer\n",
    "\n",
    "    Arg\n",
    "    ---\n",
    "    X:    image matrix with shape (m, n_H, n_W, n_C), m number of images, n_C number of convolutional layer\n",
    "    pad:  padding shape, added at both n_H and n_W\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    X_pad: padded convolutional layer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values = 0)\n",
    "\n",
    "    \n",
    "    return X_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape) #filled dimension with 0 on both side\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolution\n",
    "- Takes an input volume \n",
    "- Applies a filter at every position of the input\n",
    "- Outputs another volume (usually of different size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_step(X, W, b):\n",
    "    '''\n",
    "    Apply one filter defined by parameters W on slice of input image X of the output activation of the previous layer.\n",
    "    \n",
    "    Arg\n",
    "    ---\n",
    "    X : input data with shape (m, n , n_C)\n",
    "    W : convolutional weight (m ,n , n_C)\n",
    "    b : noise to add on convolutional layer\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    X_conv\n",
    "    \n",
    "    '''\n",
    "    X_conv = np.multiply(X, W)\n",
    "    X_conv = np.sum(X_conv)\n",
    "    X_conv = X_conv + float(b)\n",
    "    \n",
    "    \n",
    "    return X_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolution forward\n",
    "\n",
    "- what is the expected output shape\n",
    "\n",
    "- what is the number of layers after output\n",
    "\n",
    "The formulas relating the output shape of the convolution to the input shape is:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$\n",
    "\n",
    "- how to calculate backward for optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    '''\n",
    "    \n",
    "    Arg\n",
    "    ---\n",
    "    A_prev        : previous layer activation, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W             : weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b             : biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters   : python dictionary containing 'stride' and 'pad'\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    Z     : convolutional output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache : cache of values needed for the convolutional backward function\n",
    "    \n",
    "    '''\n",
    "    #get shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #get weights shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    #get hyper parameters\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    #get output shape\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1 #horizontal\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1 #vertical shape\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad) #initial padding\n",
    "    \n",
    "    #loop through all images and convolution dimensions\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooling\n",
    "\n",
    "Instead of convolution, we could also do pooling to retrive the information in each slice \n",
    "\n",
    "**Exercise**: Implement the forward pass of the pooling layer. Follow the hints in the comments below.\n",
    "\n",
    "**Reminder**:\n",
    "As there's no padding, the formulas binding the output shape of the pooling to the input shape is:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pooling_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    '''\n",
    "    \n",
    "    Arg\n",
    "    ---\n",
    "    A_prev        : previous layer activation, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters   : python dictionary containing 'stride' and 'f'\n",
    "    mode          : the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    Z     : convolutional output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache : cache of values needed for the convolutional backward function\n",
    "    \n",
    "    '''\n",
    "    #get shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #get hyper parameters\n",
    "    f = hparameters['f']\n",
    "    stride = hparameters['stride']\n",
    "    \n",
    "    #get output shape\n",
    "    n_H = int((n_H_prev - f ) / stride) + 1 #horizontal\n",
    "    n_W = int((n_W_prev - f ) / stride) + 1 #vertical shape\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    #initiate output matrix Z\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    #loop through all images and convolution dimensions\n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        Z[i, h, w, c] = np.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        Z[i, h, w, c] = np.mean(a_slice_prev)\n",
    "                    \n",
    "                    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "#print(A_prev)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pooling_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pooling_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backpropagation in DL\n",
    "\n",
    "### Convolutional layer backward pass \n",
    "\n",
    "Let's start by implementing the backward pass for a CONV layer. \n",
    "\n",
    "#### Computing dA:\n",
    "This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### Computing dW:\n",
    "This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Where $a_{slice}$ corresponds to the slice which was used to generate the acitivation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "####  Computing db:\n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**Exercise**: Implement the `conv_backward` function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = x == np.max(x)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put things together and build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_model(A, hparameter, **kwargs):\n",
    "    '''\n",
    "    TODO\n",
    "    ----\n",
    "    finish convolution function for DL\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #get shape of parameter\n",
    "    \n",
    "    #initiate\n",
    "    \n",
    "    #feature transformation\n",
    "    \n",
    "    #conv layer\n",
    "    \n",
    "    #pooling\n",
    "    \n",
    "    \n",
    "    #optimization\n",
    "    #forward - backward\n",
    "    \n",
    "    #assert shape\n",
    "    \n",
    "    #output\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and parameter tuning for DL\n",
    "- optimization\n",
    "- parameter tuning\n",
    "- best training practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the evaluation metrics?\n",
    "- Supervised\n",
    "    - Regression\n",
    "    - Classificatiohn\n",
    "- Unsupervised\n",
    "    - clustering\n",
    "    - etc\n",
    "    \n",
    "# How to make prediction?\n",
    "- Regression\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extrapolate information? \n",
    "- Final tuned parameter set\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is missing?\n",
    "- state-of-art optimization\n",
    "- new NN design for new data : blockchain network\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Reference*\n",
    "- [Opt for traning DNN](https://drive.google.com/file/d/1VKxvVy1OZbor5PjfTEQYuBzTOWu1VQwG/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
